# WebM Audio Processing Pipeline - Complete Flow Analysis

## üìä Complete Data Flow Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         BROWSER RECORDING                               ‚îÇ
‚îÇ                          (RecordingForm.jsx)                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚îú‚îÄ‚îÄ‚îÄ User clicks "Start Recording"
                   ‚îÇ
                   ‚îú‚îÄ‚îÄ‚îÄ getUserMedia() with constraints:
                   ‚îÇ    ‚Ä¢ echoCancellation: true
                   ‚îÇ    ‚Ä¢ noiseSuppression: true
                   ‚îÇ    ‚Ä¢ autoGainControl: true
                   ‚îÇ
                   ‚îú‚îÄ‚îÄ‚îÄ MediaRecorder starts capturing
                   ‚îÇ    ‚Ä¢ Codec: Vorbis (WebM container)
                   ‚îÇ    ‚Ä¢ Sample Rate: 48 kHz (browser default)
                   ‚îÇ    ‚Ä¢ Format: audio/webm MIME type
                   ‚îÇ
                   ‚îú‚îÄ‚îÄ‚îÄ ondataavailable event fires regularly
                   ‚îÇ    ‚Ä¢ Only chunks with size > 0 pushed to array
                   ‚îÇ    ‚Ä¢ audioChunksRef.current = [chunk1, chunk2, ...]
                   ‚îÇ
                   ‚îú‚îÄ‚îÄ‚îÄ User clicks "Stop Recording"
                   ‚îÇ
                   ‚îú‚îÄ‚îÄ‚îÄ Blob created on onstop event:
                   ‚îÇ    const blob = new Blob(audioChunksRef.current, 
                   ‚îÇ                           { type: 'audio/webm' })
                   ‚îÇ
                   ‚îú‚îÄ‚îÄ‚îÄ Properties at this point:
                   ‚îÇ    ‚úì blob.type = 'audio/webm'
                   ‚îÇ    ‚úì blob.size = depends on duration
                   ‚îÇ    ‚úì Codec: Vorbis @ 48 kHz
                   ‚îÇ
                   ‚îî‚îÄ‚ñ∫ recordedBlob state updated
                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ   USER CLICKS     ‚îÇ
              ‚îÇ    "ANALYZE"      ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚îú‚îÄ‚îÄ‚îÄ Blob ‚Üí File conversion:
                       ‚îÇ    const audioFile = new File(
                       ‚îÇ      [recordedBlob],
                       ‚îÇ      'recording.webm',        ‚Üê FILENAME
                       ‚îÇ      { type: 'audio/webm' }   ‚Üê MIME TYPE
                       ‚îÇ    )
                       ‚îÇ
                       ‚îú‚îÄ‚îÄ‚îÄ Frontend validation:
                       ‚îÇ    ‚úì File.name = 'recording.webm' ‚Üí ends with .webm ‚úì
                       ‚îÇ    ‚úì File.type = 'audio/webm' ‚Üí in validTypes ‚úì
                       ‚îÇ    RESULT: Validation PASSES
                       ‚îÇ
                       ‚îú‚îÄ‚îÄ‚îÄ Create FormData:
                       ‚îÇ    formData.append('file', audioFile)
                       ‚îÇ
                       ‚îú‚îÄ‚îÄ‚îÄ POST to /analyze endpoint:
                       ‚îÇ    fetch('http://127.0.0.1:8001/analyze', {
                       ‚îÇ      method: 'POST',
                       ‚îÇ      body: formData
                       ‚îÇ    })
                       ‚îÇ
                       ‚îî‚îÄ‚ñ∫ HTTP request sent to FastAPI backend
                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
               ‚îÇ  FASTAPI BACKEND        ‚îÇ
               ‚îÇ  (analyze_api.py)       ‚îÇ
               ‚îÇ  /analyze endpoint      ‚îÇ
               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚îú‚îÄ‚îÄ‚îÄ file.filename = 'recording.webm' (from FormData)
                        ‚îÇ
                        ‚îú‚îÄ‚îÄ‚îÄ File type validation:
                        ‚îÇ    if not filename.endswith('.wav' or '.mp3' or '.webm'):
                        ‚îÇ       raise HTTPException
                        ‚îÇ    else:
                        ‚îÇ       PASS ‚úì
                        ‚îÇ
                        ‚îú‚îÄ‚îÄ‚îÄ Read file bytes:
                        ‚îÇ    contents = await file.read()
                        ‚îÇ    # WebM byte stream in memory
                        ‚îÇ
                        ‚îú‚îÄ‚îÄ‚îÄ Detect format from filename:
                        ‚îÇ    file_format = 'webm'  ‚Üê detected from .webm extension
                        ‚îÇ
                        ‚îú‚îÄ‚îÄ‚îÄ Load with librosa (CRITICAL STEP):
                        ‚îÇ    waveform, _ = librosa.load(
                        ‚îÇ      io.BytesIO(contents),
                        ‚îÇ      sr=TARGET_SR,          ‚Üê 16000 Hz
                        ‚îÇ      mono=True,             ‚Üê Convert to mono
                        ‚îÇ      format='webm'          ‚Üê EXPLICIT FORMAT HINT
                        ‚îÇ    )
                        ‚îÇ
                        ‚îÇ    INPUT:  WebM bytes @ 48 kHz, Vorbis codec
                        ‚îÇ    OUTPUT: numpy array @ 16 kHz, mono, float32
                        ‚îÇ
                        ‚îú‚îÄ‚îÄ‚îÄ Create sliding window chunks:
                        ‚îÇ    ‚Ä¢ chunk_size = 3 seconds @ 16 kHz = 48,000 samples
                        ‚îÇ    ‚Ä¢ step_size = 1 second @ 16 kHz = 16,000 samples
                        ‚îÇ    ‚Ä¢ Creates overlapping 3-sec chunks with 2-sec overlap
                        ‚îÇ
                        ‚îú‚îÄ‚îÄ‚îÄ Convert chunks to PyTorch tensors:
                        ‚îÇ    for chunk in all_chunks:
                        ‚îÇ      torch.from_numpy(chunk).float()
                        ‚îÇ
                        ‚îú‚îÄ‚îÄ‚îÄ Stack into batch tensor:
                        ‚îÇ    padded_waveforms = torch.stack(all_chunks)
                        ‚îÇ    shape: (num_chunks, chunk_samples)
                        ‚îÇ
                        ‚îú‚îÄ‚îÄ‚îÄ Create attention mask:
                        ‚îÇ    input_attention_mask = (padded_waveforms != 0.0)
                        ‚îÇ    # Identifies real vs padded samples
                        ‚îÇ
                        ‚îî‚îÄ‚ñ∫ Batch ready for WavLM feature extraction
                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ  WAVLM FEATURE EXTRACTION         ‚îÇ
            ‚îÇ  (WavLMLayeredExtractor forward)  ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚îú‚îÄ‚îÄ‚îÄ Input: padded_waveforms (batch of audio chunks)
                         ‚îÇ
                         ‚îú‚îÄ‚îÄ‚îÄ WavLMModel forward pass:
                         ‚îÇ    outputs = wavlm(
                         ‚îÇ      input_values=audio,
                         ‚îÇ      attention_mask=mask
                         ‚îÇ    )
                         ‚îÇ
                         ‚îú‚îÄ‚îÄ‚îÄ Extract hidden states from 12 transformer layers:
                         ‚îÇ    stacked_layers = outputs.hidden_states[1:13]
                         ‚îÇ    shape: (12, batch_size, seq_len, 768)
                         ‚îÇ
                         ‚îú‚îÄ‚îÄ‚îÄ Compute learned weighted average:
                         ‚îÇ    normalized_weights = softmax(learnable_params)
                         ‚îÇ    weighted_avg = sum(weights * layers)
                         ‚îÇ    shape: (batch_size, seq_len, 768)
                         ‚îÇ
                         ‚îú‚îÄ‚îÄ‚îÄ Generate feature-level attention mask:
                         ‚îÇ    output_mask = get_feature_vector_attention_mask()
                         ‚îÇ
                         ‚îî‚îÄ‚ñ∫ Features ready for task-specific models
                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                                             ‚îÇ
        ‚îú‚îÄ‚ñ∫ For each of 5 tasks (block, wordrep,    ‚îÇ
        ‚îÇ   soundrep, prolongation, interjection):  ‚îÇ
        ‚îÇ                                             ‚îÇ
        ‚îî‚îÄ‚ñ∫ TASK-SPECIFIC INFERENCE                  ‚îÇ
            ‚îÇ                                         ‚îÇ
            ‚îú‚îÄ‚îÄ‚îÄ Load task model (TemporalDisfluencyNet)
            ‚îÇ    ‚Ä¢ LSTM(input_dim=768, hidden_dim=128, layers=2, bidirectional)
            ‚îÇ    ‚Ä¢ Attention mechanism
            ‚îÇ    ‚Ä¢ Binary classifier (2 output classes)
            ‚îÇ
            ‚îú‚îÄ‚îÄ‚îÄ Forward pass:
            ‚îÇ    logits, attention_weights = model(features, mask)
            ‚îÇ    shape: (batch_size, 2)  ‚Üê [prob_fluent, prob_disfluent]
            ‚îÇ
            ‚îú‚îÄ‚îÄ‚îÄ Get predictions:
            ‚îÇ    preds = argmax(logits, dim=1)  ‚Üê 0 or 1
            ‚îÇ    probs = softmax(logits)        ‚Üê confidence scores
            ‚îÇ
            ‚îú‚îÄ‚îÄ‚îÄ For each chunk predicted as disfluent (pred == 1):
            ‚îÇ    ‚îú‚îÄ Extract start/end times from chunk timing
            ‚îÇ    ‚îú‚îÄ Extract confidence score: probs[i, 1]
            ‚îÇ    ‚îú‚îÄ Extract attention weights for visualization
            ‚îÇ    ‚îî‚îÄ Create detection dict:
            ‚îÇ       {
            ‚îÇ         "type": "Block" | "WP" | "SND" | "Pro" | "Intrj",
            ‚îÇ         "start": 2.5,
            ‚îÇ         "end": 5.5,
            ‚îÇ         "confidence": 87.3,
            ‚îÇ         "attention": [weights...]
            ‚îÇ       }
            ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ Increment summary counter for task
                 
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ    AGGREGATE RESULTS      ‚îÇ
            ‚îÇ                           ‚îÇ
            ‚îú‚îÄ Collect all detections
            ‚îú‚îÄ Compute summary counts
            ‚îú‚îÄ Calculate total duration
            ‚îÇ
            ‚îî‚îÄ‚ñ∫ Return JSON response:
                {
                  "disfluencies": [
                    {
                      "type": "Block",
                      "start": 2.5,
                      "end": 5.5,
                      "confidence": 87.3,
                      "attention": [...]
                    },
                    ...
                  ],
                  "summary": {
                    "blocks": 2,
                    "wordRepetitions": 1,
                    "soundRepetitions": 0,
                    "prolongations": 1,
                    "interjections": 0
                  },
                  "duration": 45.23
                }
                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚îÇ HTTP 200 + JSON body
            ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           FRONTEND RESULT PROCESSING                                    ‚îÇ
‚îÇ           (RecordingForm.jsx ‚Üí disfluencyAnalyzer.js)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚îú‚îÄ‚îÄ‚îÄ formatAnalysisResults():
             ‚îÇ    ‚îú‚îÄ Extract disfluency counts from summary
             ‚îÇ    ‚îú‚îÄ Transform detection format for UI
             ‚îÇ    ‚îî‚îÄ Return formatted object
             ‚îÇ
             ‚îú‚îÄ‚îÄ‚îÄ Create session object:
             ‚îÇ    {
             ‚îÇ      id, patientId, date, method: 'Recording',
             ‚îÇ      audioFileName: 'Recorded Audio',
             ‚îÇ      audioURL: blob URL for playback,
             ‚îÇ      detections: formatted detections,
             ‚îÇ      duration: from analysis,
             ‚îÇ      results: { disfluencies: counts }
             ‚îÇ    }
             ‚îÇ
             ‚îú‚îÄ‚îÄ‚îÄ Store session:
             ‚îÇ    addSession(session)  ‚Üí localStorage
             ‚îÇ
             ‚îú‚îÄ‚îÄ‚îÄ Navigate to results:
             ‚îÇ    navigate('/Session', { state: { sessionId } })
             ‚îÇ
             ‚îî‚îÄ‚ñ∫ SessionForm displays detections with timeline/heatmap
                 ‚îÇ
                 ‚îî‚îÄ User can see:
                    ‚Ä¢ Timeline of detected disfluencies
                    ‚Ä¢ Confidence scores
                    ‚Ä¢ Attention heatmaps
                    ‚Ä¢ Summary statistics
```

---

## ‚úÖ Validation Checklist

### Frontend (RecordingForm.jsx)
- [x] **Blob Creation**: `new Blob(audioChunksRef.current, { type: 'audio/webm' })`
  - ‚úì MIME type correct: `audio/webm`
  - ‚úì Chunks filtered: Only size > 0 chunks included
  
- [x] **File Conversion**: `new File([recordedBlob], 'recording.webm', { type: 'audio/webm' })`
  - ‚úì Filename: `recording.webm` (ends with .webm)
  - ‚úì MIME type: `audio/webm`
  - ‚úì Constructor signature correct
  
- [x] **Validation** (disfluencyAnalyzer.js):
  ```javascript
  validTypes = ['audio/wav', 'audio/x-wav', 'audio/mpeg', 'audio/mp3', 'audio/webm']
  validExtensions = ['.wav', '.mp3', '.webm']
  ```
  - ‚úì 'audio/webm' in validTypes ‚úì
  - ‚úì '.webm' in validExtensions ‚úì
  - ‚úì Both checks pass for 'recording.webm'

- [x] **FormData Creation**:
  ```javascript
  formData.append('file', audioFile)
  ```
  - ‚úì File object properly created with name and type
  - ‚úì FormData correctly encodes binary audio data

### Backend (analyze_api.py)

- [x] **File Type Validation**:
  ```python
  if not (filename.endswith('.wav') or filename.endswith('.mp3') or filename.endswith('.webm')):
      raise HTTPException(...)
  ```
  - ‚úì Accepts `.webm` extension
  - ‚úì Error message includes WebM support

- [x] **Format Detection**:
  ```python
  if filename.endswith('.webm'):
      file_format = 'webm'
  elif filename.endswith('.mp3'):
      file_format = 'mp3'
  elif filename.endswith('.wav'):
      file_format = 'wav'
  ```
  - ‚úì Explicitly detects 'webm' format from filename
  - ‚úì Fallback to other formats or auto-detection

- [x] **librosa Loading** (CRITICAL):
  ```python
  waveform, _ = librosa.load(
      io.BytesIO(contents),
      sr=TARGET_SR,        # 16000 Hz
      mono=True,           # Mono conversion
      format=file_format   # EXPLICIT FORMAT HINT (webm)
  )
  ```
  - ‚úì Uses explicit `format='webm'` parameter
  - ‚úì Resamples to 16 kHz (TARGET_SR)
  - ‚úì Converts to mono
  - ‚úì Returns numpy float32 array

- [x] **Audio Processing Pipeline**:
  - ‚úì Chunks audio into 3-second windows
  - ‚úì 1-second step size (2-second overlap)
  - ‚úì Pads last chunk if needed
  - ‚úì Converts to PyTorch tensors
  - ‚úì Creates attention masks

- [x] **Feature Extraction** (WavLMLayeredExtractor):
  - ‚úì Loads WavLM model
  - ‚úì Extracts 12 transformer layers
  - ‚úì Computes learned weighted average
  - ‚úì Returns features (Batch, SeqLen, 768)

- [x] **Task-Specific Models**:
  - ‚úì Loads 5 models (block, wordrep, soundrep, prolongation, interjection)
  - ‚úì Binary classification (fluent vs disfluent)
  - ‚úì Extracts confidence scores
  - ‚úì Maps predictions to time ranges

- [x] **Result Aggregation**:
  - ‚úì Collects detections from all tasks
  - ‚úì Computes summary counts
  - ‚úì Includes audio duration
  - ‚úì Returns properly formatted JSON

---

## üîß Configuration Summary

| Parameter | Value | Purpose |
|-----------|-------|---------|
| **Audio Sample Rate (Browser)** | 48 kHz | MediaRecorder default |
| **Audio Sample Rate (Backend)** | 16 kHz | librosa TARGET_SR |
| **Audio Format (Browser)** | WebM + Vorbis | MediaRecorder default codec |
| **Audio Format (Filename)** | `.webm` | Explicit format hint for librosa |
| **Chunk Size** | 3 seconds | Analysis window duration |
| **Step Size** | 1 second | Overlap = 2 seconds |
| **WavLM Layers** | 12 transformer | Feature extraction depth |
| **LSTM Hidden Dim** | 128 | Task model hidden state size |
| **Output Classes** | 2 (fluent/disfluent) | Binary classification |
| **Resampling** | librosa.load() | Automatic rate conversion |

---

## üéØ Key Success Criteria

‚úÖ **Frontend to Backend**:
- Blob properly created with WebM MIME type
- File object created with .webm filename
- FormData correctly encodes binary data
- HTTP POST succeeds

‚úÖ **Backend Audio Loading**:
- librosa detects WebM format from filename
- BytesIO object properly decoded
- Waveform resampled to 16 kHz
- Mono conversion applied

‚úÖ **Feature Extraction**:
- WavLM model processes audio chunks
- 12-layer transformer outputs captured
- Weighted averaging computes features
- Output shape (Batch, SeqLen, 768)

‚úÖ **Task Models**:
- 5 models load successfully
- LSTM processes features
- Attention weights computed
- Confidence scores generated

‚úÖ **Result Delivery**:
- Detection objects with time ranges
- Summary counts aggregated
- Duration calculated
- JSON response formatted

---

## üêõ Troubleshooting Guide

### "Format not recognised" Error
**Symptoms**: `librosa.load()` fails with format error
**Cause**: BytesIO object without explicit format hint
**Solution**: ‚úÖ Already implemented with `format=file_format` parameter

### "Could not read audio file" Error
**Symptoms**: File upload succeeds but analysis fails
**Cause**: librosa cannot decode WebM bytes
**Diagnosis**: Check server logs for detailed error message
**Solution**: Ensure ffmpeg is installed or convert to WAV

### Empty Detections
**Symptoms**: Analysis completes but no disfluencies detected
**Cause**: Model predictions all negative (class 0)
**Diagnosis**: Check model confidence scores and attention weights
**Solution**: May indicate fluent speech or threshold adjustment needed

### Audio Playback Missing
**Symptoms**: Results page shows no audio player
**Cause**: audioURL not properly created from blob
**Solution**: ‚úÖ Already implemented in RecordingForm

---

## üìù Notes

- **Filename matters**: librosa format detection relies on filename extension
- **Explicit format hint**: `format='webm'` parameter is critical for BytesIO
- **Resampling**: librosa automatically handles 48 kHz ‚Üí 16 kHz conversion
- **Mono conversion**: Important for consistent feature dimensions
- **Error handling**: Frontend catches analysis errors and displays to user
- **Session storage**: Results persist in localStorage for playback

---

**Last Updated**: 2025-11-18
**Status**: ‚úÖ Complete pipeline implemented and validated
**Deployment**: Ready for production (requires ffmpeg on server for WebM support)
